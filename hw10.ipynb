{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1dba7c0d",
      "metadata": {
        "id": "1dba7c0d"
      },
      "source": [
        "# Домашнее задание № 10. Машинный перевод"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yj7aripVIsbG",
      "metadata": {
        "id": "Yj7aripVIsbG"
      },
      "source": [
        "# Задание 1 (6 баллов + 2 доп балла)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Инструкция"
      ],
      "metadata": {
        "id": "Rge8Nt8chxBx"
      },
      "id": "Rge8Nt8chxBx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Нужно обучить трансформер на том же корпусе но в другую сторону - с русского на английский.\n",
        "Можно использовать как основу первый или второй способ реализации (с MultiheadAttention или с nn.Transformer). Подберите несколько тестовых примеров для проверки обучения на каждой эпохе.\n",
        "\n",
        "Параметры ниже точно работают в колабе и модель обучается достаточно быстро. Попробуйте их немного увеличить (batch size возможно придется наоборот уменьшить). Обучайте модель хотя бы 5 эпох, а желательно больше, чтобы тестовые примеры начали переводиться более менее адекватно.\n",
        "\n",
        "После обучения возьмите хотя бы 100 примером из тестовой части параллельного корпуса и переведите их. Оцените качество переводов с помощью метрики BLEU (пример использования ниже)\n",
        "Найдите лучшие (как минимум 5) переводы согласно этой метрике и проверьте действительно ли они хорошие. Если все переводы нулевые, то пообучайте модель подольше.\n",
        "\n",
        "Чтобы получить 2 доп балла вам нужно будет придумать как оптимизировать функцию translate. Сейчас она работает только с одним текстом - это не эффективно. Можно генерировать переводы сразу для нескольких текстов (батча). Главная сложность с таким подходом состоит в том, что генерируемые тексты будут заканчиваться в разное время и нужно сделать столько итераций, сколько нужно для завершения всех текстов (т.е. условие на то, что последний токен не равен [EOS] в текущем коде не сработает).\n",
        "ВАЖНО - недостаточно просто изменить входной аргумент с text на texts и добавить еще один цикл по texts! Сама модель должна вызываться на нескольких текстах! Функция с batch prediction должна работать быстрее, поэтому переведите всю тестовую выборку и оцените качество BLEU на всех данных."
      ],
      "metadata": {
        "id": "5Lj2z5yQisCa"
      },
      "id": "5Lj2z5yQisCa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8f35e4e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8f35e4e",
        "outputId": "0a893bd5-aa65-43ec-8175-82311a058cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4548019047027907\n"
          ]
        }
      ],
      "source": [
        "# пример использования BLEU\n",
        "# обратите внимание что текты должны быть токенизированы\n",
        "import nltk\n",
        "\n",
        "hypothesis = ['It', 'is', 'a', 'cat', 'at', 'room'] # замените на перевод вашей модели\n",
        "reference = ['It', 'is', 'a', 'cat', 'inside', 'the', 'room'] # замените на эталонный перевод\n",
        "\n",
        "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, auto_reweigh=True)\n",
        "print(BLEUscore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5efa8f8",
      "metadata": {
        "id": "b5efa8f8"
      },
      "outputs": [],
      "source": [
        "# параметры которые работают в колабе\n",
        "embed_dim = 32\n",
        "num_heads = 4\n",
        "ff_dim = embed_dim*2\n",
        "num_layers = 2\n",
        "batch_size = 400"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Импорты"
      ],
      "metadata": {
        "id": "SR2jz1pi5C-j"
      },
      "id": "SR2jz1pi5C-j"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "05d202c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "05d202c4",
        "outputId": "d20c0283-a6c5-4f63-9eae-f31689f40c78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtune\n",
            "  Downloading torchtune-0.6.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting torchao\n",
            "  Downloading torchao-0.10.0-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (15 kB)\n",
            "Collecting torchdata==0.11.0 (from torchtune)\n",
            "  Downloading torchdata-0.11.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting datasets (from torchtune)\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: huggingface_hub[hf_transfer] in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.30.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.5.3)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.3.11)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.2.0)\n",
            "Collecting tiktoken (from torchtune)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting blobfile>=2 (from torchtune)\n",
            "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.21.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtune) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtune) (4.67.1)\n",
            "Collecting omegaconf (from torchtune)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from torchtune) (5.9.5)\n",
            "Requirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.11/dist-packages (from torchtune) (11.1.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.11.0->torchtune) (2.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchdata==0.11.0->torchtune) (2.32.3)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.11.0->torchtune) (2.6.0+cu124)\n",
            "Collecting pycryptodomex>=3.8 (from blobfile>=2->torchtune)\n",
            "  Downloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (5.3.2)\n",
            "Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->torchtune)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (2.2.2)\n",
            "Collecting xxhash (from datasets->torchtune)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->torchtune)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->torchtune)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (3.11.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (4.13.2)\n",
            "Collecting hf-transfer>=0.1.4 (from huggingface_hub[hf_transfer]->torchtune)\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->torchtune)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->torchtune) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.19.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.11.0->torchtune) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.11.0->torchtune) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.11.0->torchtune) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2->torchdata==0.11.0->torchtune)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2->torchdata==0.11.0->torchtune)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2->torchdata==0.11.0->torchtune)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2->torchdata==0.11.0->torchtune)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2->torchdata==0.11.0->torchtune)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2->torchdata==0.11.0->torchtune)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2->torchdata==0.11.0->torchtune)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2->torchdata==0.11.0->torchtune)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2->torchdata==0.11.0->torchtune)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2->torchdata==0.11.0->torchtune)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2->torchdata==0.11.0->torchtune) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2->torchdata==0.11.0->torchtune) (3.0.2)\n",
            "Downloading torchtune-0.6.1-py3-none-any.whl (910 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m910.7/910.7 kB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.11.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchao-0.10.0-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m120.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=fda0c38f04c39d6436baf37b631dec0024d209df91a9b851712781357e36e866\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: torchao, antlr4-python3-runtime, xxhash, pycryptodomex, omegaconf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, hf-transfer, fsspec, dill, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, blobfile, nvidia-cusolver-cu12, datasets, torchdata, torchtune\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.9.3 blobfile-3.0.0 datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 hf-transfer-0.1.9 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 omegaconf-2.3.0 pycryptodomex-3.22.0 tiktoken-0.9.0 torchao-0.10.0 torchdata-0.11.0 torchtune-0.6.1 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "9c991fbe89694fc6978d34533d57d9a6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# В колабе установите torchtune и torchao, чтобы семинарская тетрадка работала\n",
        "!pip install torchtune torchao"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers import decoders\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "from torchtune.modules import RotaryPositionalEmbeddings\n",
        "from torch.nn import Transformer\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "import nltk"
      ],
      "metadata": {
        "id": "Lx0zWNoQ5HXf"
      },
      "id": "Lx0zWNoQ5HXf",
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Данные"
      ],
      "metadata": {
        "id": "OgtHVaw33ihc"
      },
      "id": "OgtHVaw33ihc"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
        "\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWD4n0xH3h0N",
        "outputId": "bb76f49e-f45c-4681-dc13-fe7221f28cae"
      },
      "id": "vWD4n0xH3h0N",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-23 12:10:29--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 121340806 (116M)\n",
            "Saving to: ‘opus.en-ru-train.ru.2’\n",
            "\n",
            "opus.en-ru-train.ru 100%[===================>] 115.72M  20.8MB/s    in 7.0s    \n",
            "\n",
            "2025-04-23 12:10:37 (16.5 MB/s) - ‘opus.en-ru-train.ru.2’ saved [121340806/121340806]\n",
            "\n",
            "--2025-04-23 12:10:37--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67760131 (65M)\n",
            "Saving to: ‘opus.en-ru-train.en.2’\n",
            "\n",
            "opus.en-ru-train.en 100%[===================>]  64.62M  6.76MB/s    in 20s     \n",
            "\n",
            "2025-04-23 12:10:58 (3.21 MB/s) - ‘opus.en-ru-train.en.2’ saved [67760131/67760131]\n",
            "\n",
            "--2025-04-23 12:10:58--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 305669 (299K)\n",
            "Saving to: ‘opus.en-ru-test.ru.2’\n",
            "\n",
            "opus.en-ru-test.ru. 100%[===================>] 298.50K   415KB/s    in 0.7s    \n",
            "\n",
            "2025-04-23 12:10:59 (415 KB/s) - ‘opus.en-ru-test.ru.2’ saved [305669/305669]\n",
            "\n",
            "--2025-04-23 12:11:00--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 173307 (169K)\n",
            "Saving to: ‘opus.en-ru-test.en.2’\n",
            "\n",
            "opus.en-ru-test.en. 100%[===================>] 169.25K   239KB/s    in 0.7s    \n",
            "\n",
            "2025-04-23 12:11:01 (239 KB/s) - ‘opus.en-ru-test.en.2’ saved [173307/173307]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# в русскоязычных данных есть \\xa0 вместо пробелов, он может некорректно обрабатываться токенизатором\n",
        "text = open(\"opus.en-ru-train.ru\").read().replace(\"\\xa0\", \" \")\n",
        "f = open(\"opus.en-ru-train.ru\", \"w\")\n",
        "f.write(text)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "rIzlxJ6W3wBs"
      },
      "id": "rIzlxJ6W3wBs",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_sents = open(\"opus.en-ru-train.en\").read().splitlines()\n",
        "ru_sents = open(\"opus.en-ru-train.ru\").read().splitlines()\n",
        "\n",
        "print(len(en_sents))\n",
        "print(len(ru_sents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap_F2Y0c3x_c",
        "outputId": "c12eb268-fc5d-426f-9383-c0b4ae614ba1"
      },
      "id": "Ap_F2Y0c3x_c",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000000\n",
            "1000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# сократим кол-во примеров в пять раз для экономии вычислений\n",
        "\n",
        "en_sents = en_sents[:len(en_sents) // 5]\n",
        "ru_sents = ru_sents[:len(ru_sents) // 5]\n",
        "\n",
        "print(len(en_sents))\n",
        "print(len(ru_sents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpeEchSbjgQi",
        "outputId": "5e5a3dc1-1eb5-4ccb-f91f-0258920de5d9"
      },
      "id": "XpeEchSbjgQi",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200000\n",
            "200000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(zip(en_sents[:10], ru_sents[:10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuZKvNIBkQg7",
        "outputId": "54761f4c-67c7-430a-b784-6bdba6ded19a"
      },
      "id": "yuZKvNIBkQg7",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"Yeah, that's not exactly...\", 'Да, но не совсем...'),\n",
              " ('!', '!'),\n",
              " ('The schedule below is tentative; up-to-date information can be obtained at www.un.org/News/ossg/conf.htm.',\n",
              "  'Приводимое ниже расписание является предварительным; с самой последней информацией можно ознакомиться в Интернете по адресу www.un.org/News/ossg/conf.htm.'),\n",
              " ('But for now,',\n",
              "  'Но сейчас ...я вверяю вам удостовериться, что шотландцы приуменьшат'),\n",
              " (\"He'd been out there a few weeks or so.\",\n",
              "  'Они тусовались там несколько недель.'),\n",
              " (\"It'll make you feel better.\", 'Вам станет лучше.'),\n",
              " ('Come in!', 'Войдите.'),\n",
              " ('Do the math.', 'Давай, догадывайся.'),\n",
              " ('- Jenna?', '- Дженна?'),\n",
              " ('My cheekbones and beckoning pelvis already have a certain \"hello sailor\" quality to them.',\n",
              "  'Мои скулы и манящий таз уже им подают сигнал \"Привет, Матрос\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Токенайзер"
      ],
      "metadata": {
        "id": "5aMYeHmi5jns"
      },
      "id": "5aMYeHmi5jns"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "5a5bf2f6",
      "metadata": {
        "id": "5a5bf2f6"
      },
      "outputs": [],
      "source": [
        "tokenizer_ru = Tokenizer(BPE())\n",
        "tokenizer_ru.pre_tokenizer = Whitespace()\n",
        "\n",
        "# в encoder нам не нужно обозначать начало и конец поэтому единственный доп токен это паддинг\n",
        "trainer_ru = BpeTrainer(special_tokens=[\"[PAD]\"], end_of_word_suffix='</w>')\n",
        "tokenizer_ru.train(files=[\"opus.en-ru-train.ru\"], trainer=trainer_ru)\n",
        "\n",
        "tokenizer_en = Tokenizer(BPE())\n",
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "\n",
        "# в декодере добавим теги начала и конца для корректной генерации\n",
        "trainer_en = BpeTrainer(special_tokens=[\"[PAD]\", \"[BOS]\", \"[EOS]\"], end_of_word_suffix='</w>')\n",
        "tokenizer_en.train(files=[\"opus.en-ru-train.en\"], trainer=trainer_en)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_ru.decoder = decoders.BPEDecoder()\n",
        "tokenizer_en.decoder = decoders.BPEDecoder()"
      ],
      "metadata": {
        "id": "8CUvdkry5l9c"
      },
      "id": "8CUvdkry5l9c",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# раскоментируйте эту ячейку при обучении токенизатора\n",
        "# а потом снова закоментируйте чтобы при перезапуске не перезаписать токенизаторы\n",
        "\n",
        "tokenizer_ru.save('tokenizer_ru')\n",
        "tokenizer_en.save('tokenizer_en')"
      ],
      "metadata": {
        "id": "yF1hksci51Tj"
      },
      "id": "yF1hksci51Tj",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_ru = Tokenizer.from_file(\"tokenizer_ru\")\n",
        "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")"
      ],
      "metadata": {
        "id": "Utna5LLd52JL"
      },
      "id": "Utna5LLd52JL",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_ru"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAMiUE_qlRjB",
        "outputId": "176d4f8f-4c9d-4ce8-d560-4fb19c7be03b"
      },
      "id": "KAMiUE_qlRjB",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[{\"id\":0, \"content\":\"[PAD]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}], normalizer=None, pre_tokenizer=Whitespace(), post_processor=None, decoder=BPEDecoder(suffix=\"</w>\"), model=BPE(dropout=None, unk_token=None, continuing_subword_prefix=None, end_of_word_suffix=\"</w>\", fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab={\"[PAD]\":0, \"!\":1, \"\"\":2, \"#\":3, \"$\":4, \"%\":5, \"&\":6, \"'\":7, \"(\":8, \")\":9, \"*\":10, \"+\":11, \",\":12, \"-\":13, \".\":14, \"/\":15, \"0\":16, \"1\":17, \"2\":18, \"3\":19, \"4\":20, \"5\":21, \"6\":22, \"7\":23, \"8\":24, \"9\":25, \":\":26, \";\":27, \"<\":28, \"=\":29, \">\":30, \"?\":31, \"@\":32, \"A\":33, \"B\":34, \"C\":35, \"D\":36, \"E\":37, \"F\":38, \"G\":39, \"H\":40, \"I\":41, \"J\":42, \"K\":43, \"L\":44, \"M\":45, \"N\":46, \"O\":47, \"P\":48, \"Q\":49, \"R\":50, \"S\":51, \"T\":52, \"U\":53, \"V\":54, \"W\":55, \"X\":56, \"Y\":57, \"Z\":58, \"[\":59, \"\\\":60, \"]\":61, \"^\":62, \"_\":63, \"`\":64, \"a\":65, \"b\":66, \"c\":67, \"d\":68, \"e\":69, \"f\":70, \"g\":71, \"h\":72, \"i\":73, \"j\":74, \"k\":75, \"l\":76, \"m\":77, \"n\":78, \"o\":79, \"p\":80, \"q\":81, \"r\":82, \"s\":83, \"t\":84, \"u\":85, \"v\":86, \"w\":87, \"x\":88, \"y\":89, \"z\":90, \"{\":91, \"}\":92, \"~\":93, \"\":94, \"\":95, \"¡\":96, \"¢\":97, \"£\":98, ...}, merges=[(\"с\", \"т\"), (\"е\", \"н\"), (\"р\", \"а\"), (\"р\", \"о\"), (\"н\", \"о\"), (\"п\", \"о\"), (\"р\", \"е\"), (\"к\", \"о\"), (\"н\", \"и\"), (\"н\", \"а\"), (\"т\", \"о\"), (\"в\", \"о\"), (\"е\", \"л\"), (\"в\", \"а\"), (\"ен\", \"и\"), (\"р\", \"и\"), (\"н\", \"ы\"), (\"т\", \"о</w>\"), (\"г\", \"о</w>\"), (\"и\", \"т\"), (\"з\", \"а\"), (\"е\", \"р\"), (\"ц\", \"и\"), (\"к\", \"а\"), (\"с\", \"о\"), (\"д\", \"о\"), (\"м\", \"о\"), (\"л\", \"ь\"), (\"п\", \"ро\"), (\"н\", \"а</w>\"), (\"г\", \"о\"), (\"е\", \"т\"), (\"н\", \"е</w>\"), (\"е\", \"т</w>\"), (\"ст\", \"в\"), (\"е\", \"с\"), (\"д\", \"а\"), (\"с\", \"я</w>\"), (\"т\", \"а\"), (\"л\", \"а\"), (\"л\", \"и\"), (\"о\", \"б\"), (\"т\", \"ь</w>\"), (\"н\", \"о</w>\"), (\"о\", \"т\"), (\"л\", \"и</w>\"), (\"м\", \"а\"), (\"н\", \"е\"), (\"р\", \"у\"), (\"л\", \"о\"), (\"л\", \"е\"), (\"ст\", \"а\"), (\"п\", \"ри\"), (\"т\", \"и\"), (\"ч\", \"то</w>\"), (\"д\", \"и\"), (\"м\", \"е\"), (\"п\", \"ре\"), (\"ны\", \"х</w>\"), (\"м\", \"и</w>\"), (\"ени\", \"я</w>\"), (\"д\", \"а</w>\"), (\"в\", \"ы\"), (\"б\", \"о\"), (\"т\", \"е\"), (\"ч\", \"а\"), (\"с\", \"е\"), (\"в\", \"и\"), (\"в\", \"е\"), (\"с\", \"и\"), (\"д\", \"е\"), (\"ел\", \"ь\"), (\"и\", \"х</w>\"), (\"м\", \"ен\"), (\"х\", \"о\"), (\"но\", \"ст\"), (\"м\", \"и\"), (\"д\", \"у\"), (\"с\", \"у\"), (\"п\", \"е\"), (\"п\", \"а\"), (\"г\", \"а\"), (\"ч\", \"е\"), (\"п\", \"ра\"), (\"б\", \"ы\"), (\"в\", \"л\"), (\"ци\", \"и</w>\"), (\"но\", \"й</w>\"), (\"п\", \"о</w>\"), (\"ра\", \"з\"), (\"л\", \"а</w>\"), (\"но\", \"го</w>\"), (\"ени\", \"е</w>\"), (\"ст\", \"о\"), (\"е\", \"й</w>\"), (\"и\", \"с\"), (\"2\", \"0\"), (\"к\", \"и</w>\"), (\"ж\", \"е</w>\"), ...]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Препроцессинг"
      ],
      "metadata": {
        "id": "UmjezMrd6BLZ"
      },
      "id": "UmjezMrd6BLZ"
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text, tokenizer, max_len, encoder=False):\n",
        "    if encoder:\n",
        "        return tokenizer.encode(text).ids[:max_len]\n",
        "    else:\n",
        "        return [tokenizer.token_to_id('[BOS]')] + \\\n",
        "        tokenizer.encode(text).ids[:max_len] + [tokenizer.token_to_id('[EOS]')]"
      ],
      "metadata": {
        "id": "pmYlp5E16EVL"
      },
      "id": "pmYlp5E16EVL",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# важно следить чтобы индекс паддинга совпадал в токенизаторе с value в pad_sequences\n",
        "# у нас это в любом случае ноль но лучше safe than sorry\n",
        "PAD_IDX = tokenizer_en.token_to_id('[PAD]')\n",
        "PAD_IDX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhQz_m1ulHMm",
        "outputId": "fd1f21b0-5c91-44f5-c33f-4a03109e6fc2"
      },
      "id": "bhQz_m1ulHMm",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_en, max_len_ru = 47, 48"
      ],
      "metadata": {
        "id": "4ar4H2M4lOZr"
      },
      "id": "4ar4H2M4lOZr",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_en = [encode(t, tokenizer_en, max_len_en) for t in en_sents]\n",
        "X_ru = [encode(t, tokenizer_ru, max_len_ru, encoder=True) for t in ru_sents]"
      ],
      "metadata": {
        "id": "Ad1m49bL6cGD"
      },
      "id": "Ad1m49bL6cGD",
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_en), len(X_ru)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Toey6CQ1lzZ6",
        "outputId": "d912e8cf-587b-45d7-a471-a2277ec3bc6b"
      },
      "id": "Toey6CQ1lzZ6",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200000, 200000)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_ru[0]"
      ],
      "metadata": {
        "id": "qf2VlEC-9KUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7407a503-82d9-4dc7-a292-bdebbd5a33df"
      },
      "id": "qf2VlEC-9KUb",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2797, 1690, 2432, 2421, 5615, 2535]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Датасет"
      ],
      "metadata": {
        "id": "s1uyUhRfl7Q4"
      },
      "id": "s1uyUhRfl7Q4"
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, texts_ru, texts_en):\n",
        "        self.texts_en = [torch.LongTensor(sent) for sent in texts_en]\n",
        "        self.texts_en = torch.nn.utils.rnn.pad_sequence(self.texts_en, batch_first=True, padding_value=PAD_IDX)\n",
        "\n",
        "        self.texts_ru = [torch.LongTensor(sent) for sent in texts_ru]\n",
        "        self.texts_ru = torch.nn.utils.rnn.pad_sequence(self.texts_ru, batch_first=True, padding_value=PAD_IDX)\n",
        "\n",
        "        self.length = len(texts_ru)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        ids_en = self.texts_en[index]\n",
        "        ids_ru = self.texts_ru[index]\n",
        "\n",
        "        return ids_ru, ids_en"
      ],
      "metadata": {
        "id": "elqzHjOnl-As"
      },
      "id": "elqzHjOnl-As",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_ru_train, X_ru_valid, X_en_train, X_en_valid, = \\\n",
        "train_test_split(X_ru, X_en, test_size=0.05)"
      ],
      "metadata": {
        "id": "9lDsHApimBJY"
      },
      "id": "9lDsHApimBJY",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(*X_ru_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Huis5nv2mRjg",
        "outputId": "86b7d408-4b3a-46ed-90b8-892171960db8"
      },
      "id": "Huis5nv2mRjg",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3021 1678 3551 1678 4279 1690 2962 1661 3260 3717 1760 1639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = Dataset(X_ru_train, X_en_train)\n",
        "valid_set = Dataset(X_ru_valid, X_en_valid)"
      ],
      "metadata": {
        "id": "JyMsFAx5sc7h"
      },
      "id": "JyMsFAx5sc7h",
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Определения для обучения"
      ],
      "metadata": {
        "id": "p-hHXgmVmZq6"
      },
      "id": "p-hHXgmVmZq6"
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderDecoder(nn.Module): # из коробки\n",
        "    def __init__(self, vocab_size_enc, vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.embedding_enc = nn.Embedding(vocab_size_enc, embed_dim)\n",
        "        self.embedding_dec = nn.Embedding(vocab_size_dec, embed_dim)\n",
        "        self.positional_encoding = RotaryPositionalEmbeddings(embed_dim // num_heads, max_seq_len=128)\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=ff_dim,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.output_layer = nn.Linear(embed_dim, vocab_size_dec)\n",
        "\n",
        "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
        "\n",
        "        src_embedded = self.embedding_enc(src)\n",
        "        B,S,E = src_embedded.shape\n",
        "        src_embedded = self.positional_encoding(src_embedded.view(B,S,self.num_heads, E//self.num_heads)).view(B,S,E)\n",
        "\n",
        "        tgt_embedded = self.embedding_dec(tgt)\n",
        "        B,S,E = tgt_embedded.shape\n",
        "        tgt_embedded = self.positional_encoding(tgt_embedded.view(B,S,self.num_heads, E//self.num_heads)).view(B,S,E)\n",
        "\n",
        "\n",
        "        tgt_mask = (~torch.tril(torch.ones((S, S), dtype=torch.bool))).to(DEVICE)\n",
        "\n",
        "        encoder_output = self.transformer.encoder(\n",
        "            src_embedded,\n",
        "            src_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "\n",
        "        decoder_output = self.transformer.decoder(\n",
        "            tgt_embedded,\n",
        "            encoder_output,\n",
        "            tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "            memory_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "\n",
        "        output = self.output_layer(decoder_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "9nhCvf0x46zj"
      },
      "id": "9nhCvf0x46zj",
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, scheduler, run=None, print_every=200):\n",
        "\n",
        "    epoch_loss = []\n",
        "    ac = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, (texts_ru, texts_en) in enumerate(iterator):\n",
        "        texts_en = texts_en.to(DEVICE)\n",
        "        texts_ru = texts_ru.to(DEVICE)\n",
        "        texts_en_input = texts_en[:,:-1].to(DEVICE)\n",
        "        texts_en_out = texts_en[:, 1:].to(DEVICE)\n",
        "        src_padding_mask = (texts_ru == PAD_IDX).to(DEVICE)\n",
        "        tgt_padding_mask = (texts_en_input == PAD_IDX).to(DEVICE)\n",
        "        logits = model(texts_ru, texts_en_input, src_padding_mask, tgt_padding_mask)\n",
        "        optimizer.zero_grad()\n",
        "        B,S,C = logits.shape\n",
        "        loss = loss_fn(logits.reshape(B*S, C), texts_en_out.reshape(B*S))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        epoch_loss.append(loss.item())\n",
        "\n",
        "        if not (i+1) % print_every:\n",
        "            print(f'Loss: {np.mean(epoch_loss)};')\n",
        "        if run is not None:\n",
        "            run.log({\"loss\": loss.item()})\n",
        "\n",
        "    return np.mean(epoch_loss)"
      ],
      "metadata": {
        "id": "slXIWMdpsDAk"
      },
      "id": "slXIWMdpsDAk",
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion, run=None):\n",
        "\n",
        "    epoch_loss = []\n",
        "    epoch_f1 = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (texts_ru, texts_en) in enumerate(iterator):\n",
        "            texts_en = texts_en.to(DEVICE)\n",
        "            texts_ru = texts_ru.to(DEVICE)\n",
        "            texts_en_input = texts_en[:,:-1].to(DEVICE)\n",
        "            texts_en_out = texts_en[:, 1:].to(DEVICE)\n",
        "            src_padding_mask = (texts_ru == PAD_IDX).to(DEVICE)\n",
        "            tgt_padding_mask = (texts_en_input == PAD_IDX).to(DEVICE)\n",
        "\n",
        "            logits = model(texts_ru, texts_en_input, src_padding_mask, tgt_padding_mask)\n",
        "\n",
        "            B,S,C = logits.shape\n",
        "            loss = loss_fn(logits.reshape(B*S, C), texts_en_out.reshape(B*S))\n",
        "            epoch_loss.append(loss.item())\n",
        "            if run is not None:\n",
        "                run.log({\"val_loss\": loss.item()})\n",
        "\n",
        "    return np.mean(epoch_loss)"
      ],
      "metadata": {
        "id": "lgRC03aIsBBi"
      },
      "id": "lgRC03aIsBBi",
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Дополнительная функция чтобы сгенерировать перевод с нуля для текста,\n",
        "# чтобы мониторить качество\n",
        "\n",
        "@torch.no_grad\n",
        "def translate(text):\n",
        "\n",
        "\n",
        "    input_ids = tokenizer_ru.encode(text).ids[:max_len_ru]\n",
        "    output_ids = [tokenizer_en.token_to_id('[BOS]')]\n",
        "\n",
        "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(input_ids)], \\\n",
        "                                                    batch_first=True).to(DEVICE)\n",
        "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)], \\\n",
        "                                                     batch_first=True).to(DEVICE)\n",
        "\n",
        "    src_padding_mask = (input_ids_pad == PAD_IDX).to(DEVICE)\n",
        "    tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n",
        "\n",
        "    logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n",
        "\n",
        "    pred = logits.argmax(2).item()\n",
        "\n",
        "    while pred not in [tokenizer_en.token_to_id('[EOS]'), \\\n",
        "                       tokenizer_en.token_to_id('[PAD]')] and len(output_ids) < 100:\n",
        "        output_ids.append(pred)\n",
        "        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)], \\\n",
        "                                                         batch_first=True).to(DEVICE)\n",
        "        tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n",
        "\n",
        "        logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n",
        "        pred = logits.argmax(2).view(-1)[-1].item()\n",
        "\n",
        "    return tokenizer_en.decoder.decode([tokenizer_en.id_to_token(i) for i in output_ids[1:]])"
      ],
      "metadata": {
        "id": "45zM84_7q0qZ"
      },
      "id": "45zM84_7q0qZ",
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Параметры для обучения"
      ],
      "metadata": {
        "id": "hvUf_pe6tODh"
      },
      "id": "hvUf_pe6tODh"
    },
    {
      "cell_type": "code",
      "source": [
        "# попробуйте поставить параметры поменьше если в колабе обучается слишком долго!\n",
        "\n",
        "vocab_size_enc = tokenizer_ru.get_vocab_size()\n",
        "vocab_size_dec = tokenizer_en.get_vocab_size()\n",
        "\n",
        "print(vocab_size_enc, vocab_size_dec)\n",
        "\n",
        "embed_dim = 32 # еще называется D_MODEL\n",
        "num_heads = 8\n",
        "ff_dim = embed_dim * 2 # еще называется D_FF\n",
        "num_layers = 4 # количество слоев\n",
        "\n",
        "batch_size = 200\n",
        "NUM_EPOCHS = 7\n",
        "\n",
        "model = TransformerEncoderDecoder(vocab_size_enc,vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-ykOihkmd_p",
        "outputId": "43e0caaf-97c0-4642-f3d2-2d081fc6a12b"
      },
      "id": "1-ykOihkmd_p",
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000 30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_generator = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True, )\n",
        "valid_generator = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "1ZVJq1yWnQEI"
      },
      "id": "1ZVJq1yWnQEI",
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "yTc6inHsrFuj"
      },
      "id": "yTc6inHsrFuj",
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(DEVICE)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
        "\n",
        "# подбирает learning rate в процессе обучения; бывают разные скедулеры\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, \\\n",
        "    pct_start=0.05, steps_per_epoch=len(training_generator), epochs=NUM_EPOCHS)"
      ],
      "metadata": {
        "id": "s5nLnt5msOVf"
      },
      "id": "s5nLnt5msOVf",
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P2nHsYEuka7",
        "outputId": "5fd506ad-cff7-4247-8ecf-12bb21c8c461"
      },
      "id": "_P2nHsYEuka7",
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.995632 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучение"
      ],
      "metadata": {
        "id": "alyjLDzhu-Uj"
      },
      "id": "alyjLDzhu-Uj"
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate('Привет, мир!'))\n",
        "print(translate('В Черноморске гремели крыши и по улицам гуляли сквозняки.!'))\n",
        "print(translate('Что ты здесь забыл?'))\n",
        "print(translate('Мама мыла раму.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m6y3aqSygfK",
        "outputId": "34030563-e61d-4301-8587-53971dec5eeb"
      },
      "id": "4m6y3aqSygfK",
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "obstacle Italian fish Italian fish CRsimplification fish presently Spiritfish 구fish ѓ sein double DID ECLAC Facilitation DID evidenced थstability messbriefing consolidate Spiritbriefing fish School double Spiritfish SpiritSinidentical M 242 You BARfederSpiritfish 𝓲fish Spirithomproliferation briefing SpiritDemographics 실fish के linen cclinen Spirittestified linen Spirit242 Recommended fish deterconsolidate 242 disbelieve Italian fish furthering trArifish 242 fish 𝓷 homM fish fish kee echanSpiritfish linen assi242 fish M trinvfish 242 threw fronts Spiritfish furthering\n",
            "consolidate interventions fish fish fish fish conference 242 played fish fish messDemographics fish ½fish fish School fish Petersfish fish 242 BARfish fish axfish linen BARfish fish .). fish consolidate fish BAR242 fish 242 fish 242 briefing Petersinclude fish fish fish AriItalian fish fish fish fish xation obstacle fish promotional fish consolidate fish linen 242 linen sales consolidate fish stability consolidate fish fish tabernfish fish Baron fish fish abandon kee linen fish fish consolidate linen fish fish stes furthering fish Spiritfish 242 fish Sintention fish Spirithazards fish\n",
            "Verily Ann netPPP Marvin declare played consolidate consolidate consolidate ger ecological Marvin Privileges pionfish CentBARconsolidate ѓ consolidate consolidate BARcompetent consolidate Italian consolidate BARinclude declare Italian BARBAR½linen BARWSconsolidate Italian linen fish BARfish include tabernconsolidate School fish climate consolidate Privileges fish declare 242 242 fish tabernDID fish declare reshItalian ½tabernItalian fish School Ensuring _trfish declare 242 BARfish consolidate Italian Italian Spiritfish fish Privileges fish ma consolidate .). School M consolidate extrajudicial prevail fish tabernWifish fish consolidate consolidate fish\n",
            "presently Italian Italian School consolidate competent School fish abandon Italian consolidate sliding consolidate protests sein perViews Italian consolidate Italian fish Italian School fish Herdeclare Italian 242 declare School SpiritItalian trѓ fish declare Italian declare Italian .). declare consolidate fish Spiritfish Ann Italian Italian Italian prevail Wifish Italian M M stability Italian associate Lenny competent consolidate sein claimant fish include consolidate zzy Italian Italian ulum fish School School 242 Italian fish Italian Protocols declare consolidate declare declare declare Italian Italian Italian stability School fish competent fish Spiritinclude climate consolidate consolidate Italian Italian Italian 242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "run = None\n",
        "\n",
        "for epoch in tqdm(range(1, NUM_EPOCHS + 1)):\n",
        "    start_time = timer()\n",
        "    train_loss = train(model, training_generator, optimizer, loss_fn, \\\n",
        "                       scheduler, run)\n",
        "    # run.log({\"epoch_loss\": train_loss})\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(model, valid_generator, loss_fn, run)\n",
        "    # run.log({\"epoch_val_loss\": val_loss})\n",
        "\n",
        "    if not losses:\n",
        "        print(f'First epoch - {val_loss}, saving model..')\n",
        "        torch.save(model, 'model')\n",
        "\n",
        "    elif val_loss < min(losses):\n",
        "        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n",
        "        torch.save(model, 'model')\n",
        "\n",
        "    losses.append(val_loss)\n",
        "\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n",
        "           \"f\"Epoch time={(end_time-start_time):.3f}s\"))\n",
        "\n",
        "    print(translate('Привет, мир!'))\n",
        "    print(translate('В Черноморске гремели крыши и по улицам гуляли сквозняки.!'))\n",
        "    print(translate('Что ты здесь забыл?'))\n",
        "    print(translate('Мама мыла раму.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNk_sLlrvAYA",
        "outputId": "b4f33347-8a77-4124-93aa-ee3995a86350"
      },
      "id": "BNk_sLlrvAYA",
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 9.597413535118102;\n",
            "Loss: 8.319493036270142;\n",
            "Loss: 7.695869603951772;\n",
            "Loss: 7.309091204404831;\n",
            "First epoch - 5.8614000415802, saving model..\n",
            "Epoch: 1, Train loss: 7.098, Val loss: 5.861,            Epoch time=108.230s\n",
            "- You ' s !\n",
            "You ' s a way .\n",
            "What ' s you ' s ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 1/7 [01:50<11:02, 110.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You ' s a .\n",
            "Loss: 5.850856082439423;\n",
            "Loss: 5.798836179971695;\n",
            "Loss: 5.745991115570068;\n",
            "Loss: 5.706532709598541;\n",
            "Improved from 5.8614000415802 to 5.463237628936768, saving model..\n",
            "Epoch: 2, Train loss: 5.681, Val loss: 5.463,            Epoch time=109.536s\n",
            "Oh , sir !\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▊       | 2/7 [03:42<09:16, 111.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We ' re not not a little way of the way of the way of the way of the way of the way .\n",
            "What ' s you ' re going ?\n",
            "You ' re gonna be a lot .\n",
            "Loss: 5.469460718631744;\n",
            "Loss: 5.445350675582886;\n",
            "Loss: 5.421786261399587;\n",
            "Loss: 5.404422569274902;\n",
            "Improved from 5.463237628936768 to 5.265979061126709, saving model..\n",
            "Epoch: 3, Train loss: 5.393, Val loss: 5.266,            Epoch time=110.177s\n",
            "Hey , !\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 3/7 [05:35<07:27, 111.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the world .\n",
            "What ' s what you ' re going to you ?\n",
            "We ' re gonna be a lot .\n",
            "Loss: 5.270959911346435;\n",
            "Loss: 5.255839712619782;\n",
            "Loss: 5.2462620059649145;\n",
            "Loss: 5.237629261612892;\n",
            "Improved from 5.265979061126709 to 5.161215801239013, saving model..\n",
            "Epoch: 4, Train loss: 5.232, Val loss: 5.161,            Epoch time=110.225s\n",
            "Hey , !\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 4/7 [07:27<05:36, 112.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first of the same of the same of the same of the same of the same of the same of the world , and the world of the world of the world of the world .\n",
            "You ' re going to you ?\n",
            "- I ' m gonna be a lot .\n",
            "Loss: 5.15224396944046;\n",
            "Loss: 5.143813273906708;\n",
            "Loss: 5.14168472925822;\n",
            "Loss: 5.139576572775841;\n",
            "Improved from 5.161215801239013 to 5.105348243713379, saving model..\n",
            "Epoch: 5, Train loss: 5.139, Val loss: 5.105,            Epoch time=108.527s\n",
            "Hey , yeah !\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████▏  | 5/7 [09:18<03:43, 111.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first of the same of the same of the same of the same of the same of the same of the same of the world , and the world of the world .\n",
            "What are you ?\n",
            "We ' re gonna be a lot .\n",
            "Loss: 5.097927739620209;\n",
            "Loss: 5.091773506402969;\n",
            "Loss: 5.09265107234319;\n",
            "Loss: 5.089913992881775;\n",
            "Improved from 5.105348243713379 to 5.086424379348755, saving model..\n",
            "Epoch: 6, Train loss: 5.090, Val loss: 5.086,            Epoch time=109.548s\n",
            "Hey , hey !\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 6/7 [11:10<01:51, 111.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first of the same of the same of the same of the same of the same of the same of the same of the world , and the world of the world .\n",
            "What are you doing ?\n",
            "- I ' m gonna be a lot .\n",
            "Loss: 5.073873658180236;\n",
            "Loss: 5.071031522750855;\n",
            "Loss: 5.069715406099955;\n",
            "Loss: 5.073418034911156;\n",
            "Improved from 5.086424379348755 to 5.082032966613769, saving model..\n",
            "Epoch: 7, Train loss: 5.072, Val loss: 5.082,            Epoch time=108.089s\n",
            "Hey , hey !\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [13:01<00:00, 111.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first of the same of the same of the same of the same of the same of the world , and the world of the world , and the world of the world .\n",
            "What are you doing ?\n",
            "- I ' m gonna be a lot .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Оценка качества"
      ],
      "metadata": {
        "id": "_ZjcOt6IwvsQ"
      },
      "id": "_ZjcOt6IwvsQ"
    },
    {
      "cell_type": "code",
      "source": [
        "to_translate = ru_sents[2000:2500]\n",
        "ref_transl = en_sents[2000:2500]"
      ],
      "metadata": {
        "id": "rfZHUiL-wxgG"
      },
      "id": "rfZHUiL-wxgG",
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ref_transl[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_w6b1JF4_IsF",
        "outputId": "00596543-f421-4669-f72f-3dbbb4b4b100"
      },
      "id": "_w6b1JF4_IsF",
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Could you please stop staring?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "machine_transl = []\n",
        "for ex in tqdm(to_translate):\n",
        "  machine_transl.append(translate(ex))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCKs1D0e2MBo",
        "outputId": "356353bb-fde7-4f4d-90ee-a433238e042b"
      },
      "id": "zCKs1D0e2MBo",
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [01:00<00:00,  8.27it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "machine_transl[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Ghl7sPp42vsT",
        "outputId": "57f94fb1-1500-460a-92d9-257c55961559"
      },
      "id": "Ghl7sPp42vsT",
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"We ' re gonna be a lot of the same .\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = dict()\n",
        "for i in range(len(machine_transl)):\n",
        "  reference = ref_transl[i].split()\n",
        "  hypothesis = machine_transl[i].split()\n",
        "  pair = (ref_transl[i], machine_transl[i])\n",
        "  scores[pair] = (nltk.translate.bleu_score.sentence_bleu([reference], \\\n",
        "                         hypothesis, auto_reweigh=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_BHPh8T2aGN",
        "outputId": "e8059236-6c77-4a90-9e91-1b66469fed78"
      },
      "id": "r_BHPh8T2aGN",
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in sorted(scores.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "  print(*i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0qpRsGf3sxS",
        "outputId": "4b5f4b5b-0939-4538-c58f-05505bc5c227"
      },
      "id": "D0qpRsGf3sxS",
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('The time now is 04:27 PM .', 'The time now is 06 : 00 PM .') 0.3549481056010053\n",
            "('Group of 77 (briefing by Mr. Pekka Patosaari, Coordinator and Head of the Sescretariat of the United Nations Forum on Forests; and environment and sustainable development issues)', '( b ) The United Nations of the United Nations of the United Nations and the United Nations and the United Nations of the United Nations and the United Nations of the United Nations and the United Nations and the United Nations of the United Nations and the United Nations and the United Nations') 0.05697859151313651\n",
            "('What are you doing here?', 'What are you ?') 6.725854833444237e-78\n",
            "('- What are you talking about?', 'What are you ?') 5.238101011110965e-78\n",
            "('It is currently Mon May 16, 2016 2:37 pm', 'It is currently Sat May 22 , 2016 4 : 01 pm') 3.771272693679486e-78\n",
            "('It is currently Wed May 18, 2016 4:12 pm', 'It is currently Sat May 22 , 2016 4 : 01 pm') 3.771272693679486e-78\n",
            "('It should be noted that only the text kept in custody by the Secretary-General of the United Nations, in his capacity as depositary of the AGTC Agreement, constitutes the authoritative text of the Agreement.', 'The Committee was also also also also been been been been to the draft resolution - General Assembly , the Committee of the United Nations of the United Nations of the United Nations of the United Nations session of the United Nations of the United Nations of the United Nations session , and the United Nations session of the United Nations session .') 1.390226578669457e-78\n",
            "('- What am I?', '- What ?') 1.3973640466737633e-103\n",
            "('What are you?', 'What are you ?') 9.53091075863908e-155\n",
            "('- I know.', \"- I ' m sorry .\") 7.57965434483665e-155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Конечно, переводы плохие (хоть и не нулевые). Но это ожидаемо притом, что 1) я взял только часть данных, 2) я мало обучал."
      ],
      "metadata": {
        "id": "maecL8KB_7dr"
      },
      "id": "maecL8KB_7dr"
    },
    {
      "cell_type": "markdown",
      "id": "b5aa93d6",
      "metadata": {
        "id": "b5aa93d6"
      },
      "source": [
        "# Задание 2 (2 балла).\n",
        "Прочитайте главу про машинный перевод у Журафски и Маннига - https://web.stanford.edu/~jurafsky/slp3/13.pdf\n",
        "Ответьте своими словами в чем заключается техника back translation? Для чего она применяется и что позволяет получить? Опишите по шагам как ее применить к паре en->ru на данных из семинара. Сколько моделей понадобится? Сколько запусков обучения нужно будет сделать?\n",
        "\n",
        "Ответ должен содержать как минимум 10 предложений."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Суть техники backtranslation в том, что генерируются дополнительные данные для обучения модели (аугментация). Существуют и другие методы аугментации данных.\n",
        "\n",
        "Применяется, когда в паре есть малоресурсный язык (т.е. для него мало данных). Позволяет увеличить обучающую выборку.\n",
        "\n",
        "Допустим, что для русского данных больше, чем для английского, и мы хотим обучить модель en->ru.\n",
        "- Мы должны обучить модель ru->en.\n",
        "- С помощью обученной модели генерируем англ. тексты в пару к \"лишним\" русским.\n",
        "- Добавляем в обучающую выборку эти псевдо-переводные, пары с \"синтетическими\" текстами (это не та же выборка, на которой обучали первую модель).\n",
        "- Обучаем желаемую модель en->ru.\n",
        "\n",
        "Понадобится две модели, каждую запускаем один раз (при условии, что мы довольны результатом).\n",
        "\n",
        "В сеттинге backtranslation можно подбирать различные параметры модели (промежуточной, например). Кроме того, можно выбирать, какая доля обучающих примеров будет сгенерированной, можно включать несколько переводов для одной фразы и т.п.\n",
        "\n",
        "Техника работает \"удивительно хорошо\".\n",
        "\n",
        "(Jurafsky 2025: 17)"
      ],
      "metadata": {
        "id": "8J7Oega9m7zJ"
      },
      "id": "8J7Oega9m7zJ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Rge8Nt8chxBx",
        "SR2jz1pi5C-j",
        "OgtHVaw33ihc",
        "5aMYeHmi5jns",
        "UmjezMrd6BLZ",
        "s1uyUhRfl7Q4",
        "p-hHXgmVmZq6"
      ],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}